import os
import time
import json
import requests
import google.generativeai as genai
from google.cloud import pubsub_v1
from dotenv import load_dotenv

# --- Configura√ß√£o Inicial ---
load_dotenv()
print("\nüî• --- AGENTE 0: O PORTEIRO (V3.2 - Mem√≥ria + Contexto Rico) ---")

# 1. Carrega Vari√°veis
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
PROJECT_ID = os.getenv("GCP_PROJECT_ID")
TOPIC_AGENT_1 = "topic-discovery-input"

# Carrega lista de usu√°rios permitidos
ALLOWED_USERS_RAW = os.getenv("ALLOWED_USERS", "")
ALLOWED_USERS = [id.strip() for id in ALLOWED_USERS_RAW.split(",") if id.strip()]

# Modelo mantido
MODELO_PREFERIDO = "gemini-2.0-flash-lite-preview-02-05" 

# --- MEM√ìRIA VOL√ÅTIL (Salva em RAM enquanto o script roda) ---
# Estrutura: { chat_id: ["User: msg", "Bot: msg", ...] }
user_histories = {}
HISTORY_LIMIT = 6  # Mant√©m √∫ltimas 3 conversas (3 perguntas + 3 respostas)

# --- SEU TEMPLATE DE BUSCA (A ser preenchido) ---
TEMPLATE_BUSCA = """
Contexto: Empresa de m√≠dia digital com atua√ß√£o em Jobs e Marketing Mobile.

Objetivo: Prospectar {pedido}

Formato de Resposta JSON:
{{
"prospecting_request": "{pedido}",
"companies": [
{{
"name": "Nome",
"sector": "Setor",
"location": "Localiza√ß√£o",
"size": "Porte (P/M/G)",
"relevance_score": "1-10",
"website": "URL do site (obrigat√≥rio)",
"contact_points": "LinkedIn/Email Geral",
"fit_explanation": "Por que √© relevante"
}}
],
"market_insights": "Insights curtos do segmento",
"next_actions": ["a√ß√£o 1", "a√ß√£o 2"]
}}
"""

# 2. Configura Google
if not GEMINI_API_KEY:
    print("‚ùå ERRO: GEMINI_API_KEY n√£o encontrada no .env")
    exit()

if not ALLOWED_USERS:
    print("‚ö†Ô∏è AVISO: Nenhuma lista de ALLOWED_USERS configurada.")
else:
    print(f"üîí Seguran√ßa Ativa: {len(ALLOWED_USERS)} usu√°rios autorizados.")

try:
    genai.configure(api_key=GEMINI_API_KEY)
    publisher = pubsub_v1.PublisherClient()
    topic_path = publisher.topic_path(PROJECT_ID, TOPIC_AGENT_1)
    print("‚úÖ Pub/Sub configurado.")
except Exception as e:
    print(f"‚ùå Erro config: {e}")

last_update_id = 0

def get_telegram_updates(offset=None):
    url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/getUpdates"
    params = {"timeout": 10, "offset": offset}
    try:
        response = requests.get(url, params=params)
        return response.json()
    except:
        return {}

def send_telegram_message(chat_id, text):
    try:
        url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
        payload = {"chat_id": chat_id, "text": str(text)}
        requests.post(url, json=payload)
    except: pass

def update_history(chat_id, role, message):
    """Gerencia a mem√≥ria de curto prazo do usu√°rio."""
    if chat_id not in user_histories:
        user_histories[chat_id] = []
    
    # Adiciona nova mensagem
    user_histories[chat_id].append(f"{role}: {message}")
    
    # Mant√©m apenas as √∫ltimas X mensagens (Janela Deslizante)
    if len(user_histories[chat_id]) > HISTORY_LIMIT:
        user_histories[chat_id] = user_histories[chat_id][-HISTORY_LIMIT:]

def classify_intent_with_history(chat_id, current_text):
    """Analisa inten√ß√£o considerando o hist√≥rico da conversa."""
    try:
        model = genai.GenerativeModel(MODELO_PREFERIDO)
    except:
        model = genai.GenerativeModel('gemini-1.5-flash')

    # Recupera hist√≥rico formatado
    history_block = "\n".join(user_histories.get(chat_id, []))
    
    print(f"üß† Analisando hist√≥rico de {chat_id}...")
    
    prompt = f"""
    Voc√™ √© um gerente de vendas experiente. Analise o hist√≥rico de conversa e a mensagem atual.
    
    HIST√ìRICO RECENTE:
    {history_block}
    
    MENSAGEM ATUAL:
    User: {current_text}
    
    SUA MISS√ÉO:
    1. Entenda o contexto. Se o usu√°rio disse "Campinas" agora, e antes disse "Escolas", o pedido √© "Escolas em Campinas".
    2. Classifique em SEARCH ou CHAT.
    
    SA√çDA ESPERADA (JSON PURO):
    
    CASO 1: O usu√°rio quer buscar leads/empresas (SEARCH).
    {{ 
      "type": "SEARCH", 
      "consolidated_query": "Escreva aqui o termo de busca COMPLETO e MELHORADO (ex: Startups de tecnologia em Piracicaba SP)" 
    }}
    
    CASO 2: Conversa fiada, d√∫vidas, 'oi', ou falta de informa√ß√µes claras (CHAT).
    {{ 
      "type": "CHAT", 
      "response": "Sua resposta simp√°tica e curta perguntando mais detalhes." 
    }}
    """
    
    try:
        response = model.generate_content(prompt)
        text = response.text.replace('```json', '').replace('```', '').strip()
        data = json.loads(text)
        return data
    
    except Exception as e:
        print(f"‚ö†Ô∏è Erro IA: {e}")
        return {"type": "CHAT", "response": "Tive um erro de pensamento. Pode repetir o nicho?"}

def main():
    global last_update_id
    print(f"\nü§ñ Bot Agente 0 RODANDO! (Com Mem√≥ria)")
    
    while True:
        updates = get_telegram_updates(last_update_id + 1)
        
        for update in updates.get("result", []):
            last_update_id = update["update_id"]
            
            if "message" in update and "text" in update["message"]:
                chat_id = update["message"]["chat"]["id"]
                text = update["message"]["text"]
                
                # --- VERIFICA√á√ÉO DE SEGURAN√áA ---
                if str(chat_id) not in ALLOWED_USERS:
                    print(f"‚õî Acesso Negado: {chat_id}")
                    send_telegram_message(chat_id, "‚õî Acesso n√£o autorizado.")
                    continue 

                print(f"\nüì® Mensagem de {chat_id}: {text}")
                
                # 1. Decide Inten√ß√£o com Mem√≥ria
                decision = classify_intent_with_history(chat_id, text)
                tipo = decision.get('type', 'CHAT')
                
                # 2. Atualiza Hist√≥rico com o que o user disse
                update_history(chat_id, "User", text)

                if tipo == 'SEARCH':
                    # Pega a query "inteligente" que o Gemini consolidou
                    query_consolidada = decision.get('consolidated_query', text)
                    
                    print(f"ü§î Decis√£o: SEARCH -> '{query_consolidada}'")
                    send_telegram_message(chat_id, f"üîç Entendido! Preparando busca para: {query_consolidada}")
                    
                    # 3. Monta o PROMPT GIGANTE (Template)
                    # Aten√ß√£o: Usamos .format() ou f-string com cuidado por causa das chaves do JSON
                    final_prompt_content = TEMPLATE_BUSCA.format(pedido=query_consolidada)
                    
                    # 4. Manda para o Agente 1 (Perplexity)
                    payload = {
                        "command": final_prompt_content, # O Agente 1 vai receber o Prompt Inteiro aqui
                        "chat_id": chat_id,
                        "original_term": query_consolidada # √ötil para logs futuros
                    }
                    publisher.publish(topic_path, json.dumps(payload).encode("utf-8"))
                    print("üöÄ Enviado Template para Pub/Sub!")
                    
                    # Limpa hist√≥rico ap√≥s uma busca bem sucedida para evitar confus√£o no pr√≥ximo tema?
                    # Opcional. Por enquanto mantemos para contexto cont√≠nuo.
                
                else:
                    resposta = decision.get('response')
                    print(f"ü§î Decis√£o: CHAT -> '{resposta}'")
                    send_telegram_message(chat_id, resposta)
                    
                    # Atualiza hist√≥rico com a resposta do bot
                    update_history(chat_id, "Bot", resposta)

        time.sleep(1)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("Bot parado.")
